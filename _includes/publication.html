<section class="bg-wight small-padding" id="Publication">
    <div class="container">
        <div class="row">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">Selected Publication</h2>
                <hr class="primary">
            </div>
        </div>

    <div class="row">
        <div class="text-lg-start">
        <p class="text-center">[<a href="full_publication_list" style="color:#031d83;"  target="_blank"><em>see FULL publication list</em></a>]</p></div>
    </div>
    

    <div class="row">
        <div class="text-lg-start">
        <h2 class="section-heading text-uppercase">2023</h2></div>
    </div>
    <div class="row">
        <ul>

            <li><p class="text-align">H. Liu and Y. Wang, “<a href="archive/pdf/2023/2023_AICL_camera_ready.pdf" target="_blank">Towards informative few-shot prompt with maximum information gain for in-context learning</a>,” in <em>Proceedings of the 2023 Conference on Findings of the Empirical Methods in Natural Language Processing (<b>Findings of EMNLP'23</b>)</em>.</p></li>

            <li><p class="text-align">Y. Wang, W. Wei, X. Gu, X. Guan, and Y. Wang, “<a href="archive/pdf/2023/2023_TASLP_PMD_CameraReady.pdf" target="_blank">Disentangled adversarial domain adaptation for phonation mode detection in singing and speech</a>,” <i>IEEE ACM Trans. Audio Speech Lang. Process.,</i> vol. 31, pp. 3746–3759, 2023. </p></li>

            <li><p class="text-align">X. Gu, W. Zeng, and Y. Wang, “<a href="archive/pdf/2023/2023_ACM_MM2023_Fairness_Singing_camera_ready.pdf" target="_blank">Elucidate gender fairness in singing voice transcription</a>,” in <i>Proceedings of the 31st ACM International Conference on Multimedia (<b>MM 2023</b>).</i> ACM, 2023, pp. 8760–8769.</p></li>

            <li><p><p class="text-align">H. Liu, M. Shi, and Y. Wang, “<a href="archive/pdf/2023/2023_Interspeech_zero_shot_speech_assessment.pdf" target="_blank">Zero-shot automatic pronunciation assessment</a>,” in <i>24th Annual Conference of the International Speech Communication Association (<b>Interspeech 2023</b>).</i> ISCA, 2023, pp. 1009–1013.</p></li>

            <li><p><p class="text-align">J. Zhao, G. Xia, and Y. Wang, “<a href="archive/pdf/2023/2023_IJCAI_music_rearrangement.pdf" target="_blank">Q&a: Query-based representation learning for multi-track symbolic music re-arrangement</a>,” in <i>Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (<b>IJCAI 2023</b>).</i> ijcai.org, 2023, pp. 5878–5886. 
            [<a href="https://github.com/zhaojw1998/Query-and-reArrange" target="_blank" style="color:#031d83;">code</a>] 
            [<a href="https://zhaojw1998.github.io/Query_and_reArrange" target="_blank" style="color:#031d83;">demo</a>] 
            [<a href="https://colab.research.google.com/drive/1N3XeEfTCWNLTuBp9NWPwzW-hq7Ho7nQA?usp=sharing" target="_blank" style="color:#031d83;">tutorial</a>]</p></li>

            <li><p><p class="text-align">L. Ou, X. Ma, M.-Y. Kan, and Y. Wang, “<a href="archive/pdf/2023/2023_ACL_Lyric_Translation.pdf" target="_blank">Songs across borders: Singable and controllable neural lyric translation</a>,” in <i>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (<b>ACL 2023</b>).</i> Association for Computational Linguistics, 2023, pp. 447–467. 
            [<a href="https://github.com/Sonata165/ControllableLyricTranslation" target="_blank" style="color:#031d83;">code</a>] 
            [<a href="https://www.oulongshen.xyz/lyric_translation" target="_blank" style="color:#031d83;">demo</a>]</p></li>

            <li><p><p class="text-align">Y. Wang, W. Wei, and Y. Wang, “<a href="archive/pdf/2023/2023_ICASSP_phonation_mode.pdf" target="_blank">Phonation mode detection in singing: A singer adapted model</a>,” in <i>IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP 2023</b>).</i> IEEE, 2023, pp. 1–5.</p></li>

            <li><p class="text-align">S. Dai, X. Ma, Y. Wang, and R. B. Dannenberg, “<a href="https://arxiv.org/abs/2105.04709" target="_blank">Personalised popular music generation using imitation and structure</a>,” <i>Journal of New Music Research,</i> vol. 51, no. 1, pp. 69–85, 2023. </p></li>
        </ul>
    </div>


    <div class="row">
        <div class="text-lg-start">
        <h2 class="section-heading text-uppercase">2022</h2></div>
    </div>
    <div class="row">
    <ul>
        <li><p class="text-align">W. Wei, H. Huang, X. Gu, H. Wang, and Y. Wang, “<a href="archive/pdf/2022/2022_TMLR_camera_ready.pdf" target="_blank">Unsupervised
            mismatch localization in cross-modal sequential data with application to mispronunciations localization</a>,” <i>Transactions on Machine Learning Research,</i> vol. 2022, 2022. </p></li>

        <li><p class="text-align">X. Wu, H. Huang, Y. Ding, H. Wang, Y. Wang, and Q. Xu, “<a href="archive/pdf/2022/2022_AAAI2023_FedNP_hengguan.pdf" target="_blank">Fednp: Towards non-iid federated learning via federated neural propagation</a>,” in <i>Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI 2023).</i> AAAI Press, 2023, pp. 10399–10407.</p></li>

        <li><p class="text-align">H. Huang, X. Gu, H. Wang, C. Xiao, H. Liu, and Y. Wang, “<a href="archive/pdf/2022/2022_ECBNN_full.pdf" target="_blank">Extrapolative continuous-time bayesian neural network for fast training-free test-time adaptation</a>,” in <i>Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022 (<b>NeurIPS 2022</b>).</i> 2022.</p></li>

        <li><p class="text-align">L. Ou, X. Gu, and Y. Wang, “<a href="archive/pdf/2022/2022_ALT_ISMIR_2022_Camera_ready 8.23.pdf" target="_blank">Transfer learning of wav2vec 2.0 for automatic lyric transcription</a>,” in <i>Proceedings of the 23rd International Society for Music Information Retrieval Conference (<b>ISMIR 2022</b>).</i> 2022, pp. 891–899. </p></li>

        <li><p class="text-align">X. Ma, X. Liu, B. Zhang, and Y. Wang, “<a href="archive/pdf/2022/2022_Robust_Melody_Track_Identification_in_Symbolic_Music_Camera Ready.pdf" target="_blank">Robust melody track identification in symbolic music</a>,” in <i>Proceedings of the 23rd International Society for Music Information Retrieval Conference (<b>ISMIR 2022</b>).</i> 2022, pp. 842–849.</p></li>

        <li><p class="text-align">J. Zhao, G. Xia, and Y. Wang, “<a href="archive/pdf/2022/2022_Beat Transformer.pdf" target="_blank">Beat transformer: Demixed beat and downbeat tracking with dilated self-attention</a>,” in <i>Proceedings of the 23rd International Society for Music Information Retrieval Conference (<b>ISMIR 2022</b>).</i> 2022, pp. 169–177. 
        [<a href="https://github.com/zhaojw1998/Beat-Transformer" target="_blank" style="color:#031d83;">code</a>] 
        [<a href="https://colab.research.google.com/drive/1IdrpMO1AivWmy-Bm8ktmMy14ED9jllux?usp=sharing" target="_blank" style="color:#031d83;">tutorial</a>] 
        [<a style="color:#031d83;" href="https://zhaojw1998.github.io/presentation_page#Beat-Transformer" target="_blank">video</a>]</p></li>

        <li><p class="text-align">J. Zhao, G. Xia, and Y. Wang, “<a href="archive/pdf/2022/2022_Adversarial VAE.pdf" target="_blank">Domain adversarial training on conditional variational auto-encoder for controllable music generation</a>,” in <i>Proceedings of the 23rd International Society for Music Information Retrieval Conference (<b>ISMIR 2022</b>).</i> 2022, pp. 925–932. 
        [<a href="https://github.com/zhaojw1998/DAT-CVAE" target="_blank" style="color:#031d83;">code</a>] 
        [<a href="https://zhaojw1998.github.io/DAT_CVAE" target="_blank" style="color:#031d83;">demo</a>] 
        [<a style="color:#031d83;" href="https://zhaojw1998.github.io/presentation_page#DAT-CVAE" target="_blank">video</a>]</p></li>

        <li><p class="text-align">X. Gu, L. Ou, D. Ong, and Y. Wang, “<a href="archive/pdf/2022/2022_ACM_MM_MM-ALT.pdf" target="_blank">MM-ALT: A multimodal automatic lyric transcription system</a>,” in <i>Proceedings of the 30th ACM International Conference on Multimedia (<b>MM 2022</b>).</i> ACM, 2022, pp. 3328–3337. (<b>Top Paper Award</b>)
        [<a href="https://n20em.github.io/" target="_blank" style="color:#031d83;">demo</a>] </p></li>

        <li><p class="text-align">X. Ma, Y. Wang, and Y. Wang, “<a href="archive/pdf/2022/2022_ACM_MM_User_Preference_Modeling.pdf" target="_blank">Content based user preference modeling in music generation</a>,” in <i>Proceedings of the 30th ACM International Conference on Multimedia (<b>MM 2022</b>).</i> ACM, 2022, pp. 2473–2482. 
        [<a href="archive/demo/stan/2022_MusicRx-C_Badinerie_music_generated_0.6_0.6_0.6_0.6.mp3" target="_blank" style="color:#031d83;">demo 1</a>] 
        [<a href="archive/demo/stan/2022_MusicRx-C_Imagine_music_generated_100_0.6_0.6_1.0_0.6.mp3" target="_blank" style="color:#031d83;">demo 2</a>]</p></li>

        <li><p>L. Ou, Z. Guo, E. Benetos, J. Han, and Y. Wang, “<a href="archive/pdf/2022/2022_ICASSP_APT.pdf" target="_blank">Exploring transformer’s potential on automatic piano transcription</a>,” in <i>IEEE International Conference on Acoustics, Speech and Signal Processing (<b>ICASSP 2022</b>).</i> IEEE, 2022, pp. 776–780. </p></li>
    </ul>
    </div>
    

    <div class="row">
        <div class="text-lg-start">
        <h2 class="section-heading text-uppercase">2021</h2></div>
    </div>
    <div class="row">
    <ul>
        <li><p>X. Ma, Y. Wang, M. Kan, and W. S. Lee, “<a href="archive/pdf/2019-2021/2021_AI-lyricist.pdf" target="_blank">Ai-lyricist: Generating music and vocabulary constrained lyrics</a>,” in <i>Proceedings of the 29th ACM International Conference on Multimedia (<b>MM 2021</b>).</i> ACM, 2021, pp. 1002–1011.
        [<a href="archive/demo/stan/2021_AI-Lyricist_Imagine_lyrics_generated.pdf" target="_blank" style="color:#031d83;">lyrics demo</a>] 
        [<a href="archive/demo/stan/2021_AI-Lyricist_Imagine_lyrics_generated.mp3" target="_blank" style="color:#031d83;">synthsis demo</a>]</p></li>
            
        <li><p>H. Huang, H. Liu, H. Wang, C. Xiao, and Y. Wang, “<a href="archive/pdf/2019-2021/2021_Neural-ODE-meets-a-sampling-free-latent-point-process.pdf" target="_blank">STRODE: Stochastic boundary ordinary differential equation</a>,” in <i>Proceedings of the 38th International Conference on Machine Learning (<b>ICML 2021</b>).</i> PMLR, 2021, pp. 4435–4445. 
        [<a href="https://github.com/Waffle-Liu/STRODE" target="_blank" style="color:#031d83;">code</a>] 
        [<a href="archive/pdf/2019-2021/2021_ICML2021_STRODE_slides.pdf" target="_blank" style="color:#031d83;">slides</a>]</p></li>         
    </ul>
    </div>
    

    <div class="row">
        <div class="text-lg-start">
        <p class="text-center">[<a href="full_publication_list" style="color:#031d83;"  target="_blank"><em>see FULL publication list</em></a>]</p></div>
    </div>

    </div>
















    </div>
</section>
